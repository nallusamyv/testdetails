import pandas as pd

def batch_grouped_data(df, batch_size):
    # Extract the first word from 'field_name' using split on underscore
    df['first_word'] = df['field_name'].str.split('_').str[0]
    
    # Group by 'table_name' and 'first_word' and collect the groups
    grouped = df.groupby(['table_name', 'first_word'])
    groups = [group for _, group in grouped]
    
    # Sort groups by size in descending order to optimize packing
    groups.sort(key=lambda g: len(g), reverse=True)
    
    batches = []
    current_batch = []
    current_count = 0
    
    for group in groups:
        group_size = len(group)
        
        # If the group is larger than the batch_size, add it as a separate batch
        if group_size > batch_size:
            batches.append(group)
            continue
        
        # Check if adding this group would exceed the current batch size
        if current_count + group_size > batch_size:
            if current_batch:
                # Combine the current_batch groups into a single DataFrame
                batches.append(pd.concat(current_batch, ignore_index=True))
                current_batch = []
                current_count = 0
        
        # Add the group to the current batch
        current_batch.append(group)
        current_count += group_size
    
    # Add any remaining groups in the current_batch
    if current_batch:
        batches.append(pd.concat(current_batch, ignore_index=True))
    
    return batches

# Example usage:
# Assuming df is your DataFrame with 'table_name' and 'field_name' columns
# batches = batch_grouped_data(df, batch_size=1000)
# for batch in batches:
#     process(batch)  # Replace with your processing function
