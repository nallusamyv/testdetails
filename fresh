    .config("spark.sql.catalog.hadoop_catalog.catalog-impl", "org.apache.iceberg.hadoop.HadoopCatalog") \
    .config("spark.sql.catalog.hadoop_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
    .getOrCreate()

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, DecimalType, IntegerType, FloatType, StringType, DateType
from decimal import Decimal
import datetime

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Writing Parquet File") \
    .getOrCreate()

# Define schema for the DataFrame with one field for each data type
schema = StructType([
    StructField("credit_card_number", StringType(), True),
    StructField("card_type", StringType(), True),
    StructField("expiry_date", StringType(), True),
    StructField("cardholder_name", StringType(), True),
    StructField("cvv", IntegerType(), True),
    StructField("transaction_amount", DecimalType(10, 2), True),
    StructField("transaction_date", DateType(), True),
    StructField("latitude", FloatType(), True),
    StructField("longitude", FloatType(), True),
    StructField("partition_dt", StringType(), True),
    StructField("contxt_id", StringType(), True)
])

# Sample data with proper conversions
data = [
    ("**** **** **** 1234", "Visa", "05/26", "John Doe", 123, Decimal("100.50"), datetime.date(2024, 5, 16), 40.7128, -74.0060, "2024-05-16", "context1"),
    ("**** **** **** 5678", "Mastercard", "07/25", "Jane Smith", 456, Decimal("200.75"), datetime.date(2024, 5, 16), 34.0522, -118.2437, "2024-05-16", "context2")
]

# Convert data into a list of tuples
data_converted = [
    (record[0], record[1], record[2], record[3], int(record[4]), Decimal(record[5]), datetime.date.fromisoformat(record[6]), float(record[7]), float(record[8]), record[9], record[10])
    for record in data
]

# Create a DataFrame
df = spark.createDataFrame(data_converted, schema)

# Write DataFrame to Parquet file partitioned by partition_dt and contxt_id
df.write
