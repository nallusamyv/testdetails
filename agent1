import os
import json
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableParallel
from langchain.schema.output_parser import StrOutputParser
import logging

# Import OpenAI token function
from your_openai_module import getopenaillm  

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Token-Aware LLM Wrapper using getopenaillm()
class TokenAwareLLM:
    def __init__(self):
        self.token = None
        self.refresh_token()

    def refresh_token(self):
        """Fetch a new token using getopenaillm()"""
        self.token = getopenaillm()
        logger.info("Token refreshed successfully")

    def generate(self, prompt):
        if not self.token:
            self.refresh_token()
        return json.dumps({
            "logical_name": f"Logical_{hash(prompt)}",
            "logical_description": f"Processed {prompt[:20]}..."
        })

# Setup Processing Chain (Only Logical)
def setup_chain():
    logical_prompt = PromptTemplate.from_template(
        "Analyze column metadata and return approved logical naming.\n"
        "Input Column: {column_name}\nDescription: {description}\n"
        "Respond ONLY with valid JSON: {{'logical_name': '...', 'logical_description': '...'}}"
    )

    llm = TokenAwareLLM()
    output_parser = StrOutputParser()

    return logical_prompt | llm.generate | output_parser

# Process Single Row
def process_row(chain, row):
    try:
        result = chain.invoke({
            "column_name": row["column_name"],
            "description": row["description"]
        })
        
        return {
            **row,
            **json.loads(result)
        }
    except Exception as e:
        logger.error(f"Processing failed for row {row.name}: {str(e)}")
        return {**row, "error": str(e)}

# Process Batch in Parallel
def process_batch(batch_df):
    chain = setup_chain()
    results = [process_row(chain, row) for _, row in batch_df.iterrows()]
    return results

# Split Data into Batches Based on Dataset Name
def split_batches(df, num_batches=5):
    """Split dataset while keeping similar dataset names together."""
    df_sorted = df.sort_values(by=["dataset_name", "column_name"])  # Ensure grouping
    return np.array_split(df_sorted, num_batches)

# Main Parallel Processing Function
def process_csv_parallel(input_path, output_path):
    df = pd.read_csv(input_path)

    # Split into 5 batches
    batches = split_batches(df, num_batches=5)

    results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        batch_results = list(executor.map(process_batch, batches))

    # Flatten results
    for batch in batch_results:
        results.extend(batch)

    result_df = pd.DataFrame(results)
    result_df.to_csv(output_path, index=False)
    return result_df

# Example Usage
if __name__ == "__main__":
    input_csv = "input_data.csv"
    output_csv = "processed_results.csv"
    
    if not os.path.exists(input_csv):
        raise FileNotFoundError(f"Input file {input_csv} not found")
        
    print(f"Starting parallel processing of {input_csv}")
    result = process_csv_parallel(input_csv, output_csv)
    print(f"Completed processing. Results saved to {output_csv}")
    print(f"Successfully processed {len(result) - result['error'].isna().sum()} rows")
