import pandas as pd
import json
import logging
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from tenacity import retry, stop_after_attempt, wait_exponential
from concurrent.futures import ThreadPoolExecutor
from pydantic import BaseModel, ValidationError

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('metadata_generation.log'), logging.StreamHandler()]
)

class ColumnMetadata(BaseModel):
    logical_name: str
    logical_description: str

class LLMProcessor:
    def __init__(self):
        self.chain = None
        self._initialize_llm()
        
    def _initialize_llm(self) -> None:
        self.model = ChatOpenAI(model="gpt-4")
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "Generate business-friendly metadata. Include SNo in JSON response."),
            ("human", "SNo: {SNo}\nPhysical: {ColumnName}\nTechnical: {ColumnDescription}")
        ])
        self.chain = prompt_template | self.model | StrOutputParser()
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))
    def process_batch(self, batch: pd.DataFrame) -> list:
        try:
            # Process batch with SNo included
            return self.chain.batch(batch[['SNo', 'ColumnName', 'ColumnDescription']].to_dict('records'))
        except Exception as e:
            logging.error(f"Processing failed: {str(e)}")
            raise

def process_csv(input_path: str, output_path: str, max_workers: int = 3) -> None:
    processor = LLMProcessor()
    
    try:
        df = pd.read_csv(input_path)
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(0, len(df), 5):
                batch = df.iloc[i:i+5]
                futures.append(executor.submit(
                    processor.process_batch,
                    batch[['SNo', 'ColumnName', 'ColumnDescription']].copy()
                ))

            for future in futures:
                try:
                    batch_results = future.result()
                    for result in batch_results:
                        try:
                            parsed = json.loads(result)
                            validated = ColumnMetadata(**{k:v for k,v in parsed.items() if k != 'SNo'})
                            results.append({
                                'SNo': parsed['SNo'],  # Preserve SNo from input
                                **validated.dict()
                            })
                        except (KeyError, json.JSONDecodeError, ValidationError) as e:
                            logging.error(f"Invalid response format: {str(e)}")
                except Exception as e:
                    logging.error(f"Batch failed: {str(e)}")

        # Merge using SNo as key
        output_df = df.merge(pd.DataFrame(results), on='SNo', how='left')
        output_df.to_csv(output_path, index=False)
        logging.info(f"Processed {len(output_df)} records")
        
    except Exception as e:
        logging.critical(f"Critical error: {str(e)}")
        if results:
            pd.DataFrame(results).to_csv(f"partial_{output_path}", index=False)
        raise

if __name__ == "__main__":
    process_csv(
        input_path="input.csv",
        output_path="output.csv",
        max_workers=3
    )
