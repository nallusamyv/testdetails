
import pandas as pd
from langchain.schema import SystemMessage, HumanMessage
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain.llms import OpenAI
from langchain.schema.runnable import RunnableParallel
from langchain.memory import ConversationBufferMemory
import time

# Step 1: Token Management
class TokenManager:
    def __init__(self, token_initializer):
        self.token_initializer = token_initializer
        self.access_token = self.token_initializer()
        self.last_refresh_time = time.time()

    def get_token(self):
        # Refresh token if it has expired (e.g., after 1 hour)
        if time.time() - self.last_refresh_time > 3600:  # 1 hour expiration
            self.access_token = self.token_initializer()
            self.last_refresh_time = time.time()
        return self.access_token

# Example token initializer function (replace with your actual logic)
def initialize_token():
    print("Refreshing access token...")
    return "your_new_access_token"

# Initialize token manager
token_manager = TokenManager(initialize_token)

# Step 2: Initialize LLM with Token Refresh
def initialize_llm():
    return OpenAI(
        openai_api_key=token_manager.get_token(),  # Use the refreshed token
        temperature=0.7
    )

# Step 3: Define Memory for Previous Results
memory = ConversationBufferMemory(memory_key="chat_history")

# Step 4: Define Prompt Templates with Memory
# Template for generating a logical name
logical_name_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Generate a logical name for: {input}"),
])

# Template for generating a description
description_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Generate a description for: {input}"),
])

# Step 5: Define LLM Chains with Memory
# Chain for generating a logical name
logical_name_chain = LLMChain(
    llm=initialize_llm(),  # Initialize LLM with token refresh
    prompt=logical_name_template,
    output_key="logical_name",
    memory=memory  # Add memory to the chain
)

# Chain for generating a description
description_chain = LLMChain(
    llm=initialize_llm(),  # Initialize LLM with token refresh
    prompt=description_template,
    output_key="description",
    memory=memory  # Add memory to the chain
)

# Step 6: Use RunnableParallel to Run Chains in Parallel
# Define the parallel runnable
parallel_runnable = RunnableParallel(
    logical_name=logical_name_chain,
    description=description_chain
)

# Step 7: Load the Input CSV File
input_csv_path = "input.csv"  # Replace with your input CSV file path
output_csv_path = "output.csv"  # Replace with your desired output CSV file path

# Load the CSV file into a DataFrame
df = pd.read_csv(input_csv_path)

# Step 8: Process Each Row and Generate Results
results = []

for index, row in df.iterrows():
    input_data = {"input": row["input"]}  # Replace "input" with the column name in your CSV

    try:
        # Invoke the parallel runnable
        result = parallel_runnable.invoke(input_data)
        results.append({
            "input": row["input"],
            "logical_name": result["logical_name"],
            "description": result["description"]
        })
    except Exception as e:
        print(f"Error processing row {index}: {e}")
        # Retry with a refreshed token
        print("Retrying with a refreshed token...")
        logical_name_chain.llm = initialize_llm()  # Reinitialize LLM
        description_chain.llm = initialize_llm()  # Reinitialize LLM
        result = parallel_runnable.invoke(input_data)
        results.append({
            "input": row["input"],
            "logical_name": result["logical_name"],
            "description": result["description"]
        })

# Step 9: Save Results to Output CSV File
output_df = pd.DataFrame(results)
output_df.to_csv(output_csv_path, index=False)

print(f"Results saved to {output_csv_path}")
