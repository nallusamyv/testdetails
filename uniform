from pyspark.sql import SparkSession
from delta import DeltaTable

# Initialize Spark with Glue integration
spark = SparkSession.builder \
    .appName("DeltaUniFormGlue") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.glue_catalog.warehouse", "s3://parquetbuckettesting-east-2/iceberg/icebergcatalog/") \
    .config("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog") \
    .config("spark.sql.catalog.glue_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .config("spark.hadoop.hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .config("spark.sql.hive.metastore.glue.defaultWarehouseDir", "s3://parquetbuckettesting-east-2/delta-warehouse/") \
    .getOrCreate()

# 1. Create Delta table with Iceberg compatibility
parquet_path = "s3://parquetbuckettesting-east-2/parquetfile/customerbank/"
df = spark.read.parquet(parquet_path)

delta_path = "s3://parquetbuckettesting-east-2/iceberg/uniform-table/customerbank"
iceberg_metadata_path = f"{delta_path}/_iceberg_metadata"

# Write data as Delta with UniForm enabled
(df.write
  .format("delta")
  .option("path", delta_path)
  .option("delta.enableIcebergCompatV2", "true")
  .option("delta.columnMapping.mode", "name")
  .option("delta.universalFormat.enabledFormats", "iceberg")
  .saveAsTable("customerbank"))  # Now uses Glue Catalog via Hive integration

# 2. Register Iceberg metadata
spark.sql(f"""
CREATE TABLE glue_catalog.deltadb.customerbank 
USING iceberg
LOCATION '{iceberg_metadata_path}'
""")

# 3. Verify access
# As Delta table via Glue
spark.sql("SELECT * FROM customerbank").show(5)

# As Iceberg table via Glue
spark.sql("SELECT * FROM glue_catalog.deltadb.customerbank").show(5)
