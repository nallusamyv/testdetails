import os
import json
import logging
import pandas as pd
from typing import List, Dict, Optional
from pydantic import BaseModel, ValidationError
from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from tenacity import retry, stop_after_attempt, wait_exponential

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    CHUNK_SIZE = 10  # Number of rows per API call
    MAX_RETRIES = 3
    MODEL_NAME = "gpt-3.5-turbo-0125"
    REQUIRED_COLUMNS = ["DatasetName", "DatasetDescription", "ColumnName", "ColumnDescription", "Datatype"]

# Pydantic Models
class ProcessedRow(BaseModel):
    DatasetName: str
    DatasetDescription: Optional[str] = None
    ColumnName: str
    ColumnDescription: Optional[str] = None
    Datatype: Optional[str] = None

# Processing Chain
class CSVProcessor:
    def __init__(self, api_key: str):
        self.llm = ChatOpenAI(
            model=Config.MODEL_NAME,
            temperature=0,
            openai_api_key=api_key
        )
        self.chain = self._create_chain()
        self.column_mapping = {}

    def _create_chain(self):
        prompt = """Process these data rows with inconsistent column names:
        
        Original Columns: {columns}
        
        For each row:
        1. Map values to these standard columns:
           - DatasetName (table/source name)
           - DatasetDescription (table/source description)
           - ColumnName (column identifier)
           - ColumnDescription (description + valid + default values)
           - Datatype (data type with length if available)
        
        2. Combine description fields:
           - Format: "[Description] | Valid: [values] | Default: [value]"
        
        3. Format datatype:
           - "type(length)" if length exists (e.g. "varchar(255)")
           - Just type otherwise
        
        Return ONLY a JSON array with objects using these keys:
        {required_columns}
        
        Input Data:
        {rows}
        """

        return (
            RunnableLambda(self._format_input)
            | RunnableLambda(lambda x: prompt.format(**x))
            | self.llm
            | StrOutputParser()
            | RunnableLambda(self._parse_output)
        )

    def _format_input(self, input_data: dict) -> dict:
        return {
            "columns": json.dumps(input_data["columns"]),
            "rows": json.dumps(input_data["rows"], indent=2),
            "required_columns": json.dumps(Config.REQUIRED_COLUMNS)
        }

    @retry(stop=stop_after_attempt(Config.MAX_RETRIES), wait=wait_exponential(multiplier=1, min=2, max=10))
    def _parse_output(self, response: str) -> List[Dict]:
        try:
            data = json.loads(response)
            return [self._validate_row(row) for row in data]
        except (json.JSONDecodeError, ValidationError) as e:
            logger.error(f"Parsing failed: {str(e)}")
            raise

    def _validate_row(self, row: Dict) -> Dict:
        try:
            return ProcessedRow(**row).dict()
        except ValidationError as e:
            logger.warning(f"Validation error: {str(e)}")
            return {k: None for k in Config.REQUIRED_COLUMNS}

    def process_csv(self, input_path: str, output_path: str) -> None:
        """Main processing workflow"""
        df = pd.read_csv(input_path)
        self._analyze_columns(df.columns)
        
        processed_chunks = []
        for chunk in self._chunk_dataframe(df):
            try:
                result = self.chain.invoke({
                    "columns": list(df.columns),
                    "rows": chunk
                })
                processed_chunks.extend(result)
            except Exception as e:
                logger.error(f"Chunk processing failed: {str(e)}")
                processed_chunks.extend([{}] * len(chunk))

        self._save_results(processed_chunks, output_path)

    def _analyze_columns(self, columns: List[str]) -> None:
        """Map original columns to standard names using LLM"""
        prompt = f"""Map these columns to standard names:
        {columns}
        
        Standard names: {Config.REQUIRED_COLUMNS}
        
        Return JSON mapping: {{"original": "standard"}}"""
        
        try:
            response = self.llm.invoke(prompt).content
            self.column_mapping = json.loads(response)
            logger.info(f"Column mapping established: {self.column_mapping}")
        except Exception as e:
            logger.error(f"Column mapping failed: {str(e)}")
            self.column_mapping = {}

    def _chunk_dataframe(self, df: pd.DataFrame) -> List[List[Dict]]:
        """Split dataframe into chunks"""
        return [df[i:i+Config.CHUNK_SIZE].to_dict('records') 
                for i in range(0, len(df), Config.CHUNK_SIZE)]

    def _save_results(self, data: List[Dict], output_path: str) -> None:
        """Save processed data to CSV"""
        df = pd.DataFrame(data)
        df = df[Config.REQUIRED_COLUMNS]  # Enforce column order
        df.to_csv(output_path, index=False)
        logger.info(f"Saved processed data to {output_path}")

# Usage
if __name__ == "__main__":
    processor = CSVProcessor(api_key=os.getenv("OPENAI_API_KEY"))
    processor.process_csv(
        input_path="input_data.csv",
        output_path="processed_data.csv"
    )
